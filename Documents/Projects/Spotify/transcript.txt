Having a Big Bang is one of the most common causes of data science project failures, and you probably have done it at least a couple of times. In this episode, we will show you why it is often better to aim for suboptimal solutions at the start of a project and how you can avoid the Big Bang problem by following an ancient Japanese philosophy. By the way, we are rebroadcasting this episode because it is one of our favorite early episodes and the content can be very valuable to our new listeners. Enough said. Enjoy this episode. This week we are going to talk about another mental model that we found very useful again and again in our experience leading data science projects and teams. We'll begin with a problem that you can solve. I think it's common to see in business. We start approaching a data intensive problem, data initial planning. We gather people from different disciplines together, all the way from designers, product people, engineers, to data scientists. And then we have some initial directions to go with. And then three months down the line, boom. There's a big delivery, project is done, and everyone is surprised by what was delivered, because what was delivered is kind of what everybody thinks. They agreed at the beginning, but not exactly the same. So this is the big bang problem. This happens a lot because of doing one big iteration and the same mentality of thinking about one big iteration can happen on a lot of different levels. When you're solving problems, the root cause could be that when you ask a good problem solver or people who are interested in problem solving to find a solution for something, we often want to go for the best solution that there is. We often think about all the possible details about the problem that we are solving, all the edge cases that there are around the problem that we're having, and want to come up with a solution that is solid, that is robust, that deals with all the cases, because that's kind of also representing the kind of work that we're doing. And in these situations, what can be problematic is focusing on that optimal solution and working kind of in the dark before getting feedback, before getting measurements from the environment about the status of your solution and about where you're going. Both of these problems could be improved if you find a mental model and if you find the tools that can allow you to intentionally maybe be suboptimal but go end to end. And I believe the mental model that we want to introduce is going to help you to move to that direction. Yes, absolutely. I mentioned that from the business perspective, you mentioned that from the daily work and the actual data science was the challenge you're facing there. And if you look at one obstruction level higher, this is very fundamentally an issue of delayed feedback cycle. In both cases, you have a feedback cycle that is too long for it to be useful. That always creates surprises and further delays to make things shorter. Everything else being equal, everyone can work double time, but then that doesn't tend to work out really well in knowledge intensive work. Or the other solution is you intentionally aim for the less optimal solution, like you mentioned. And that brings us to the mental model we want to introduce, which is Wabisabi. I'm East Asian, so I'll try to explain it as much as I can. Wabisabi originally is a concept that comes from the Japanese aesthetics and Japanese philosophy as a contrast to the Greek philosophy and aesthetic, which is based on perfection, based on symmetry, based on balance. The Japanese Wabi Sabi philosophy takes a different perspective, which is everything in life is imperfect, impermanent and incomplete. And therefore there is beauty in creations that are imperfect, incomplete and impermanent. Most of us think this is a very beautiful concept that can actually be used as a mental model to address the two problems we just saw. So what that really means is that as a problem solver, you look for solutions that don't cover all the edge cases, but then that allows you to get to that solution relatively quick and then use that to get feedback and then further improve on that. And by this feedback, maybe it's good if you're clear it's not only getting feedback from business, but we also mean by this is getting feedback from your problem domain, getting feedback from the environment, getting feedback from your data. How well does the model with these characteristics explain the problem that you're facing? Can fit the data that you have? How well does it fit the nature of the problem that you're solving? And you could spend a lot of time theorizing about this. You could spend a lot of time sharpening and polishing the solution that you have. But on the other hand, with accepting babysavi or maybe with looking up to babysabi thinking as one of your mental models, you'd be free to try out suboptimal solutions, each of which might just give you a sense of some dimension and some aspects of the problem you're dealing with. So instead of working on this giant, perfect solution, you might be able to at the same time try out five different solutions, each of which might be focused on answering a specific question, giving you insights and information about a specific aspect of the problem you're dealing with. By that mentality, you do get information much faster about the problem that you're solving. Can you give some examples of what you just mentioned? In a lot of cases, we are diving into data. We're digging into data to compute, to extract information from this data. And by definition, the information that we extract is a compression of your data. It's some form of aggregation of your data. Otherwise you just have all the data in front of you. You also have in most cases that the data that you're working with has a lot of noise. It has cases in it which might be better to left out. In these situations, one type of thinking is to remove all the noises, is to publish your data as much as possible, so that the insights that you extract from it are the cleanest. But then in practice, the first time that you're asking your questions, the first time that you're extracting that information from your data, the noises might play a role, but you might still identify the main message or the main insight that you want to find out from your data. In this case, vibesavi thinking could be helpful in ignoring the noises, acknowledging the noises, acknowledging uncertainty, but still being able to learn from your data is still being able to extract that information that lets you go the next step, that's one dimension and that's data and the noise and maybe cleaning up the data. Another aspect is also the technical dimensions of your solution. We all know we don't want to make bad code. That kind of thinking is definitely valuable. But depending on the stage of work that you are on your problems, it might be of less or more importance. You could be dealing with a situation where you need about one or two weeks to properly fit your code into the existing pieces of code and libraries and other systems that are maybe even in your organization to extract the data, process it and then output it and put it in the proper place. On the other hand, you might have an option to extract this data in however way you want, quickly run some models on it and see if these models make any sense or not. Again, that's another dimension where depending on where you are, it might be useful to use some kind of Avisabi thinking to work intentionally with suboptimal pieces of code that answer your question and let you decide maybe there's nothing in this direction to look into and save you two or three weeks of very clean, very effective solid coding. Or maybe on the other hand let you know that there is definitely something here and then guide you to actually doing it in a proper way and investing more time in cleaning up your solution. And I think from the business side it's super important that you are aware of this and you also encourage your team, your data scientists to do this as well because this at the end, what it will give you is to allow you to reduce the uncertainty quite a lot. And you got much fast feedback loops that you can already integrated into your product strategy, into your business strategy, much earlier in your consideration of those. Then you have to wait for a three month project that needs to be finished and you got one big result at the end. We are both big fans of the thinking of course when you apply this there are a few things to pay attention to because you are intentionally doing things they are imperfect. One of the first thing is that if you are not used to this way of thinking then it can be quite difficult to get started. So there I think a practical way to start is to zoom out one level and ask yourself now I look at my work from one level higher where does imperfection matter? If I look at this part of my work does imperfection matter here? How much does it matter? If I look at this part of my work, does imperfection matter? How much does it matter? And if I go for imperfection, how much faster can I go? And that gives you information to really make trade off and also when you do this it allows you to see where perfection really really matters and that is something very important for you to keep in mind as well because that means, you know, you need to reserve more time and iteration round for that part of the program. Seeing the big picture allows you to connect every piece of work that you're doing to the final goal that you're trying to achieve and it also brings in perspective the importance of the step that you're working on. In many cases, especially when you're working with pipelines of data that are processing huge amounts of data and coming up with a single compress model or a few decisions there are many places where you really do not need perfection. There are many places where your systems and your solutions are very tolerant of noise as long as the noise lets you have a representative picture of the system that you're working with. It's also important that you are very aware of the shortcuts that you are going to take or you have taken and be very disciplined about it. Yeah, definitely that could become the downside of what Israeli when you're developing solutions you can make a lot of shortcuts and even those solutions end up to be accepted in some sense they make it into your production environment or hopefully way before that. You really need to be careful about the babysabi decisions that you made and see if any of them should actually be carried over to your production environment and that's where probably a lot of the technical debt in data science and machine learning work can also be created or is at least historically created. Yeah, I think it's super important to have an overview of those and also when you are prioritizing future work to really take those into consideration because the natural tendency is not to do it. However, if you apply, if you start with five imperfect solutions you probably will end up with one that is actually the direction you want to go. So that also means you don't need to pay back the technical debt in the other four directions. Yeah, in a way you don't invest a lot or you might be able to save yourself from investing in directions that you will not follow in the future. But then babysabe really needs to be accompanied always by discipline, by the discipline of cleaning up the solutions and making sure that you don't carry over your imperfections, the ones that are especially hurtful into a production system and then just waiting for a disaster to happen in the future. It's really a two sided way. If you do not have discipline afterwards, it might be very dangerous, of course, to keep using wabi sabi or overusing wabisabi thinking in your solution. Yeah. So definitely pay attention to that. And last but not least, there's one more point that you should really pay attention to when you use Barbiesabi. When you communicate this to other people who might not be so familiar with data science work, or might not be so familiar with this way of thinking, you really want to set the right expectation. In our experience, some key communication principles are quite important. One is to mention that we are working on an imperfect solution consciously right now. But the goal is that in the next situation this will get better and the next situation it will be better. And these are the approximate timelines. By doing this, you give people a much more balanced perspective. That's very interesting and probably one of the most sensitive parts of applying wishes to be a you mentioned the case where you have to make awareness that this is imperfect and it's going to improve. So you're making awareness about your intentional choices of imperfection. But there's also another dangerous side to this. You go with an imperfect solution, especially when the imperfections are a lot under the hood. You show your results to the stakeholders, to the business, to whoever is interested in collaborating with you or using and applying them and then they're actually quite happy with the work. And that causes also another mismatch of expectations, the one where you are aware of the imperfections and you put them there on purpose to get an idea, to get quick feedback. But those imperfections are hidden from a lot of other people. The mismatching expectations is caused by their thinking that you already have a good solution and you're knowing that you really don't have a good solution. That's I think, a more tricky case to navigate. I think it's important to have a conversation with the teams that will use your data driven solution to openly discuss this, to say, okay, these are the shortcuts we have taken. These are the things we plan that we are going to fix later. And I know you're super happy with this decision, but here is what will happen or here are the consequences if we don't fix those things, if we don't do further iterations, at least in my experience, the adjacent teams, the sponsors of the project. The stakeholders, they're usually quite receptive to this kind of information and tend to be quite supportive in terms of giving you the space and time to do this. And sometimes they come up with good questions. For example, what is the impact of those issues, how much value it is to fix the issue? And I think those are also very valid questions that you should look into and also make a judgment of. We realize this is an imperfection. Yes, we choose not to deal with it. Yes, we say we are going to deal with it later, but given everything we know now, does it still make sense to do it? That is something you do want to discuss and get different perspectives and linking to the previous point. Even if the decision at that point was okay, we decided not to do it. It's super important to keep a record of that decision so that some point in the future, if this issue comes out, you still know why it's there and you still have some idea on how to fix it. I really like your suggestion. It's a good case for rational thinking. You propose a solution with imperfections. If there's a misconception from the user site that this is good enough, they should be good explanations available to make it clear why this is not good enough or why this could go really wrong unless you fix those imperfections. If you have those answers, they should be convincing and if you don't have any answers, then maybe baby sabi is good enough. Yeah, I think we cover a lot. So, tradition of the show, take away what would be one takeaway that our listeners can use tomorrow? Maybe because I made a few mistakes. Actually it's in applying web services and not necessarily having the required discipline to follow up and to do the correction. My takeaway would be to take a look at some of the most important projects that you've done and see if you can identify those babiesabi kind of decisions there and see if you've carried some of them already to a stage where they shouldn't have been carried, or check to see if you did have the discipline to polish them and put them in proper production form. Yeah, I would also like to suggest one takeaway from my side. Take the most complex project or the project that's going to take most of your time, or bring power that you have at hand and just zoom out and ask, does imperfection matter? How much does it matter and which part does it matter more than others? I think if you have not done that before, you'll be really surprised. So everybody, thank you very much. This is another episode of Naked Data Science. This time the mental model is a little bit advanced. So then please do make sure you understand the two sides of wabisabi that we mentioned. Just be a little bit more conscious when you applied it. But then, at least in our experience, it's a very important tool that can make a huge difference, especially for high uncertainty, big scope projects. All right, everybody, take care. See you next time. See you next time. Just one last thing before you go. If you are not a data scientist yet but want to become one, you should really attend our webinar. We will demystify the transition into data science. We'll show you the most effective way to build your skills, and we will advise you on the four possible options you can take to go from where you are to landing a data science job in as little as nine months. Find out more at NDS showwebinar. That is NDS. Alright, that's the end of this episode. Have a nice day.